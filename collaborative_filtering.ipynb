{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col, explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/27 14:03:36 WARN Utils: Your hostname, miray resolves to a loopback address: 127.0.1.1; using 172.24.33.252 instead (on interface wlo1)\n",
      "23/01/27 14:03:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/mahdi/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mahdi/.ivy2/jars\n",
      "org.apache.hbase#hbase-shaded-mapreduce added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a7697366-6d9e-4e22-ae8e-c587c2f9e402;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.hbase#hbase-shaded-mapreduce;2.4.14 in local-m2-cache\n",
      "\tfound org.slf4j#slf4j-api;1.7.33 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.2.0-incubating in central\n",
      "\tfound jakarta.annotation#jakarta.annotation-api;1.3.5 in central\n",
      "\tfound jakarta.validation#jakarta.validation-api;2.0.2 in central\n",
      "\tfound org.glassfish.hk2.external#jakarta.inject;2.6.1 in central\n",
      "\tfound org.javassist#javassist;3.25.0-GA in central\n",
      "\tfound org.apache.hadoop#hadoop-distcp;2.10.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;2.10.0 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      ":: resolution report :: resolve 201ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tjakarta.annotation#jakarta.annotation-api;1.3.5 from central in [default]\n",
      "\tjakarta.validation#jakarta.validation-api;2.0.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;2.10.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-distcp;2.10.0 from central in [default]\n",
      "\torg.apache.hbase#hbase-shaded-mapreduce;2.4.14 from local-m2-cache in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.2.0-incubating from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.glassfish.hk2.external#jakarta.inject;2.6.1 from central in [default]\n",
      "\torg.javassist#javassist;3.25.0-GA from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.33 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a7697366-6d9e-4e22-ae8e-c587c2f9e402\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/6ms)\n",
      "23/01/27 14:03:38 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "23/01/27 14:03:40 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.apache.hbase_hbase-shaded-mapreduce-2.4.14.jar added multiple times to distributed cache.\n",
      "23/01/27 14:03:40 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.slf4j_slf4j-api-1.7.33.jar added multiple times to distributed cache.\n",
      "23/01/27 14:03:40 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.apache.htrace_htrace-core4-4.2.0-incubating.jar added multiple times to distributed cache.\n",
      "23/01/27 14:03:40 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/jakarta.annotation_jakarta.annotation-api-1.3.5.jar added multiple times to distributed cache.\n",
      "23/01/27 14:03:40 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/jakarta.validation_jakarta.validation-api-2.0.2.jar added multiple times to distributed cache.\n",
      "23/01/27 14:03:40 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.glassfish.hk2.external_jakarta.inject-2.6.1.jar added multiple times to distributed cache.\n",
      "23/01/27 14:03:40 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.javassist_javassist-3.25.0-GA.jar added multiple times to distributed cache.\n",
      "23/01/27 14:03:40 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.apache.hadoop_hadoop-distcp-2.10.0.jar added multiple times to distributed cache.\n",
      "23/01/27 14:03:40 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.apache.hadoop_hadoop-annotations-2.10.0.jar added multiple times to distributed cache.\n",
      "23/01/27 14:03:40 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.apache.yetus_audience-annotations-0.5.0.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('collaborative_filtering').master('yarn').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Put the data into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/homework4': File exists\n",
      "put: `/user/homework4/games.csv': File exists\n",
      "put: `/user/homework4/ratings.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user/homework4\n",
    "!hdfs dfs -put games.csv /user/homework4/games.csv\n",
    "!hdfs dfs -put ratings.csv /user/homework4/ratings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Read the data and ceate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read and parse the data\n",
    "games = spark.read.csv(\"/user/homework4/games.csv\", header=True, inferSchema=True)\n",
    "ratings = spark.read.csv(\"/user/homework4/ratings.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------+--------------------+--------------------+\n",
      "|game_id|                name|release_date|             summary|          meta_score|\n",
      "+-------+--------------------+------------+--------------------+--------------------+\n",
      "|      1|The Legend of Zel...|   23-Nov-98|As a young boy, L...|                  99|\n",
      "|      2|Tony Hawk's Pro S...|   20-Sep-00|As most major pub...|                  98|\n",
      "|      3| Grand Theft Auto IV|   29-Apr-08|\"[Metacritic's 20...| fresh off the bo...|\n",
      "|      4|         SoulCalibur|    8-Sep-99|This is a tale of...|                  98|\n",
      "|      5|  Super Mario Galaxy|   12-Nov-07|[Metacritic's 200...|                  97|\n",
      "+-------+--------------------+------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "games.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|game_id|user_id|rating|\n",
      "+-------+-------+------+\n",
      "|      1|    314|     5|\n",
      "|      1|    439|     3|\n",
      "|      1|    588|     5|\n",
      "|      1|   1169|     4|\n",
      "|      1|   1185|     4|\n",
      "+-------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Check the missing values\n",
    "\n",
    "We see number of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(981548, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "game_id    981548\n",
       "user_id    981548\n",
       "rating     981548\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ratings.toPandas()\n",
    "print(df.shape)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are no missing values in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Split the data into train and test\n",
    "\n",
    "We give 80% of the data to train and 20% to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Normalize the data\n",
    "\n",
    "We normalize the data to have a better performance. First, we create a vector assembler to combine the features into a single vector. Then, we create a normalizer to normalize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create the normalizer objects\n",
    "assembler = VectorAssembler(inputCols=[\"rating\"], outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledRating\", withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(assembler.transform(training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|game_id|user_id|    rating|\n",
      "+-------+-------+----------+\n",
      "|      1|    588| 1.1611812|\n",
      "|      1|   1169|0.14537998|\n",
      "|      1|   1185|0.14537998|\n",
      "|      1|   2077|0.14537998|\n",
      "|      1|   2487|0.14537998|\n",
      "+-------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Normalize the training data\n",
    "training = scalerModel.transform(assembler.transform(training))\n",
    "training = training.drop(\"features\")\n",
    "\n",
    "# convert scaledRating to a float column\n",
    "training = training.withColumn(\"scaledRating\", vector_to_array(\"scaledRating\").getItem(0).cast(\"float\"))\n",
    "training = training.drop(\"rating\")\n",
    "training = training.withColumnRenamed(\"scaledRating\", \"rating\")\n",
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+\n",
      "|game_id|user_id|     rating|\n",
      "+-------+-------+-----------+\n",
      "|      1|    314|  1.1611812|\n",
      "|      1|    439|-0.87042123|\n",
      "|      1|   3922|  1.1611812|\n",
      "|      1|  10140| 0.14537998|\n",
      "|      1|  10335| 0.14537998|\n",
      "+-------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Normalize the test data\n",
    "test = scalerModel.transform(assembler.transform(test))\n",
    "test = test.drop(\"features\")\n",
    "\n",
    "# convert scaledRating to a float column\n",
    "test = test.withColumn(\"scaledRating\", vector_to_array(\"scaledRating\").getItem(0).cast(\"float\"))\n",
    "test = test.drop(\"rating\")\n",
    "test = test.withColumnRenamed(\"scaledRating\", \"rating\")\n",
    "test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Create the ALS model\n",
    "\n",
    "We create this model with the following parameters:\n",
    "\n",
    "* maxIter: 5 (we use 5 iterations to train the model)\n",
    "* regParam: 0.01 (we use regularization to avoid overfitting)\n",
    "* coldStartStrategy: \"drop\" (we drop the missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id\", itemCol=\"game_id\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Evaluate the model\n",
    "\n",
    "We evaluate the model with the RMSE (Root Mean Squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.174760009038952\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Make some recommendations for 2 users and 2 items\n",
    "\n",
    "Here we can see the model can do both item-based and user-based recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|     recommendations|\n",
      "+-------+--------------------+\n",
      "|      1|[{2302, 0.2719504...|\n",
      "|      3|[{4772, 4.970701}...|\n",
      "+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 123:=====================================================>(99 + 1) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|game_id|     recommendations|\n",
      "+-------+--------------------+\n",
      "|      1|[{46305, 5.828892...|\n",
      "|      3|[{34723, 9.687718...|\n",
      "+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generate top 10 game recommendations for each user and each item\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "gameRecs = model.recommendForAllItems(10)\n",
    "userRecs.show(2)\n",
    "gameRecs.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - Create functions to make recommendations for specific users and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_user(user_id, num_recommendations):\n",
    "    user = ratings.filter(ratings.user_id == user_id).select(\"user_id\").distinct()\n",
    "    recommendations = model.recommendForUserSubset(user, num_recommendations)\n",
    "    recommendations = recommendations.select(explode(\"recommendations\").alias(\"recommendations\"))\n",
    "    recommendations = recommendations.select(\"recommendations.game_id\", \"recommendations.rating\")\n",
    "    return recommendations.join(games, recommendations.game_id == games.game_id).select(games.game_id, games.name, recommendations.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_item(game_id, num_recommendations):\n",
    "    game = ratings.filter(ratings.game_id == game_id).select(\"game_id\").distinct()\n",
    "    recommendations = model.recommendForItemSubset(game, num_recommendations)\n",
    "    recommendations = recommendations.select(explode(\"recommendations\").alias(\"recommendations\"))\n",
    "    recommendations = recommendations.select(\"recommendations.user_id\", \"recommendations.rating\")\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some exmaples of recommendations for specific users and items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------+\n",
      "|game_id|                name|   rating|\n",
      "+-------+--------------------+---------+\n",
      "|   3550|            Deathrow|2.6548383|\n",
      "|   7104|         Risk System|2.5164847|\n",
      "|   5330|         Tetris Axis|2.2537565|\n",
      "|   5318|Majin and the For...|2.1659617|\n",
      "|   9497|           Velocibox|2.1539714|\n",
      "+-------+--------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recommend_for_user(10140, 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 188:============================================>         (83 + 2) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|user_id|   rating|\n",
      "+-------+---------+\n",
      "|  46305| 5.828892|\n",
      "|  51328| 5.551867|\n",
      "|  41205| 5.319552|\n",
      "|  45633|4.9871006|\n",
      "|   3740|4.8282356|\n",
      "|  42854|4.5203958|\n",
      "|   4800|4.4558706|\n",
      "|  10207|4.4459324|\n",
      "|  34246| 4.420498|\n",
      "|   6919|4.4186625|\n",
      "+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recommend_for_item(1, 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
