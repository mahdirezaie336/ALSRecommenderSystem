{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col, explode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/27 00:57:37 WARN Utils: Your hostname, miray resolves to a loopback address: 127.0.1.1; using 172.24.33.252 instead (on interface wlo1)\n",
      "23/01/27 00:57:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/mahdi/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mahdi/.ivy2/jars\n",
      "org.apache.hbase#hbase-shaded-mapreduce added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6f6d44b2-58e3-45df-a79f-ec9d0e9cccde;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hbase#hbase-shaded-mapreduce;2.4.14 in local-m2-cache\n",
      "\tfound org.slf4j#slf4j-api;1.7.33 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.2.0-incubating in central\n",
      "\tfound jakarta.annotation#jakarta.annotation-api;1.3.5 in central\n",
      "\tfound jakarta.validation#jakarta.validation-api;2.0.2 in central\n",
      "\tfound org.glassfish.hk2.external#jakarta.inject;2.6.1 in central\n",
      "\tfound org.javassist#javassist;3.25.0-GA in central\n",
      "\tfound org.apache.hadoop#hadoop-distcp;2.10.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;2.10.0 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      ":: resolution report :: resolve 208ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tjakarta.annotation#jakarta.annotation-api;1.3.5 from central in [default]\n",
      "\tjakarta.validation#jakarta.validation-api;2.0.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;2.10.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-distcp;2.10.0 from central in [default]\n",
      "\torg.apache.hbase#hbase-shaded-mapreduce;2.4.14 from local-m2-cache in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.2.0-incubating from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.glassfish.hk2.external#jakarta.inject;2.6.1 from central in [default]\n",
      "\torg.javassist#javassist;3.25.0-GA from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.33 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6f6d44b2-58e3-45df-a79f-ec9d0e9cccde\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/5ms)\n",
      "23/01/27 00:57:39 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "23/01/27 00:57:41 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.apache.hbase_hbase-shaded-mapreduce-2.4.14.jar added multiple times to distributed cache.\n",
      "23/01/27 00:57:41 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.slf4j_slf4j-api-1.7.33.jar added multiple times to distributed cache.\n",
      "23/01/27 00:57:41 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.apache.htrace_htrace-core4-4.2.0-incubating.jar added multiple times to distributed cache.\n",
      "23/01/27 00:57:41 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/jakarta.annotation_jakarta.annotation-api-1.3.5.jar added multiple times to distributed cache.\n",
      "23/01/27 00:57:41 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/jakarta.validation_jakarta.validation-api-2.0.2.jar added multiple times to distributed cache.\n",
      "23/01/27 00:57:41 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.glassfish.hk2.external_jakarta.inject-2.6.1.jar added multiple times to distributed cache.\n",
      "23/01/27 00:57:41 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.javassist_javassist-3.25.0-GA.jar added multiple times to distributed cache.\n",
      "23/01/27 00:57:41 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.apache.hadoop_hadoop-distcp-2.10.0.jar added multiple times to distributed cache.\n",
      "23/01/27 00:57:41 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.apache.hadoop_hadoop-annotations-2.10.0.jar added multiple times to distributed cache.\n",
      "23/01/27 00:57:41 WARN Client: Same path resource file:///home/mahdi/.ivy2/jars/org.apache.yetus_audience-annotations-0.5.0.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('collaborative_filtering').getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Put the data into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/homework4': File exists\n",
      "put: `/user/homework4/games.csv': File exists\n",
      "put: `/user/homework4/ratings.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user/homework4\n",
    "!hdfs dfs -put games.csv /user/homework4/games.csv\n",
    "!hdfs dfs -put ratings.csv /user/homework4/ratings.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Read the data and ceate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read and parse the data\n",
    "games = spark.read.csv(\"/user/homework4/games.csv\", header=True, inferSchema=True)\n",
    "ratings = spark.read.csv(\"/user/homework4/ratings.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------+--------------------+--------------------+\n",
      "|game_id|                name|release_date|             summary|          meta_score|\n",
      "+-------+--------------------+------------+--------------------+--------------------+\n",
      "|      1|The Legend of Zel...|   23-Nov-98|As a young boy, L...|                  99|\n",
      "|      2|Tony Hawk's Pro S...|   20-Sep-00|As most major pub...|                  98|\n",
      "|      3| Grand Theft Auto IV|   29-Apr-08|\"[Metacritic's 20...| fresh off the bo...|\n",
      "|      4|         SoulCalibur|    8-Sep-99|This is a tale of...|                  98|\n",
      "|      5|  Super Mario Galaxy|   12-Nov-07|[Metacritic's 200...|                  97|\n",
      "+-------+--------------------+------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "games.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|game_id|user_id|rating|\n",
      "+-------+-------+------+\n",
      "|      1|    314|     5|\n",
      "|      1|    439|     3|\n",
      "|      1|    588|     5|\n",
      "|      1|   1169|     4|\n",
      "|      1|   1185|     4|\n",
      "+-------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Check the missing values\n",
    "\n",
    "We see number of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(981548, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "game_id    981548\n",
       "user_id    981548\n",
       "rating     981548\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ratings.toPandas()\n",
    "print(df.shape)\n",
    "df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are no missing values in the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Split the data into train and test\n",
    "\n",
    "We give 80% of the data to train and 20% to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Normalize the data\n",
    "\n",
    "We normalize the data to have a better performance. First, we create a vector assembler to combine the features into a single vector. Then, we create a normalizer to normalize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create the normalizer objects\n",
    "assembler = VectorAssembler(inputCols=[\"rating\"], outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledRating\", withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(assembler.transform(training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+\n",
      "|game_id|user_id|scaledRating|\n",
      "+-------+-------+------------+\n",
      "|      1|    314|    1.161791|\n",
      "|      1|    439| -0.87061214|\n",
      "|      1|    588|    1.161791|\n",
      "|      1|   2077|  0.14558943|\n",
      "|      1|   2487|  0.14558943|\n",
      "+-------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Normalize the training data\n",
    "training = scalerModel.transform(assembler.transform(training))\n",
    "training = training.drop(\"features\")\n",
    "\n",
    "# convert scaledRating to a float column\n",
    "training = training.withColumn(\"scaledRating\", vector_to_array(\"scaledRating\").getItem(0).cast(\"float\"))\n",
    "training = training.drop(\"rating\")\n",
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+\n",
      "|game_id|user_id|scaledRating|\n",
      "+-------+-------+------------+\n",
      "|      1|   1169|  0.14558943|\n",
      "|      1|   1185|  0.14558943|\n",
      "|      1|   3922|    1.161791|\n",
      "|      1|   9246|  -2.9030154|\n",
      "|      1|  18313|    1.161791|\n",
      "+-------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Normalize the test data\n",
    "test = scalerModel.transform(assembler.transform(test))\n",
    "test = test.drop(\"features\")\n",
    "\n",
    "# convert scaledRating to a float column\n",
    "test = test.withColumn(\"scaledRating\", vector_to_array(\"scaledRating\").getItem(0).cast(\"float\"))\n",
    "test = test.drop(\"rating\")\n",
    "test.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Create the ALS model\n",
    "\n",
    "We create this model with the following parameters:\n",
    "\n",
    "* maxIter: 5 (we use 5 iterations to train the model)\n",
    "* regParam: 0.01 (we use regularization to avoid overfitting)\n",
    "* coldStartStrategy: \"drop\" (we drop the missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id\", itemCol=\"game_id\", ratingCol=\"scaledRating\", coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Evaluate the model\n",
    "\n",
    "We evaluate the model with the RMSE (Root Mean Squared Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.180493926604111\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"scaledRating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Make some recommendations for 2 users and 2 items\n",
    "\n",
    "Here we can see the model can do both item-based and user-based recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|     recommendations|\n",
      "+-------+--------------------+\n",
      "|      1|[{5568, 2.3973413...|\n",
      "|      3|[{6728, 7.729961}...|\n",
      "+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 123:===================================================>  (96 + 2) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|game_id|     recommendations|\n",
      "+-------+--------------------+\n",
      "|      1|[{9112, 5.0123577...|\n",
      "|      3|[{51328, 16.04025...|\n",
      "+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Generate top 10 game recommendations for each user and each item\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "gameRecs = model.recommendForAllItems(10)\n",
    "userRecs.show(2)\n",
    "gameRecs.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_user(user_id, num_recommendations):\n",
    "    user = ratings.filter(ratings.user_id == user_id).select(\"user_id\").distinct()\n",
    "    recommendations = model.recommendForUserSubset(user, num_recommendations)\n",
    "    recommendations = recommendations.select(explode(\"recommendations\").alias(\"recommendations\"))\n",
    "    recommendations = recommendations.select(\"recommendations.game_id\", \"recommendations.rating\")\n",
    "    return recommendations.join(games, recommendations.game_id == games.game_id).select(games.game_id, games.name, recommendations.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_item(game_id, num_recommendations):\n",
    "    game = ratings.filter(ratings.game_id == game_id).select(\"game_id\").distinct()\n",
    "    recommendations = model.recommendForItemSubset(game, num_recommendations)\n",
    "    recommendations = recommendations.select(explode(\"recommendations\").alias(\"recommendations\"))\n",
    "    recommendations = recommendations.select(\"recommendations.user_id\", \"recommendations.rating\")\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------+\n",
      "|game_id|                name|   rating|\n",
      "+-------+--------------------+---------+\n",
      "|   5568|      Darwin Project|2.3973413|\n",
      "|     53|   BioShock Infinite|2.1700134|\n",
      "|   2742|Metal Gear Solid ...|2.0701506|\n",
      "|    420|      Dark Souls III|  2.02298|\n",
      "|   2136|Space Invaders Ex...|1.9544657|\n",
      "|   4154|Azure Striker Gun...| 1.863071|\n",
      "|   9769| Redux: Dark Matters|1.8579313|\n",
      "|     96|Street Fighter Al...|1.8452163|\n",
      "|    224|        Mario Tennis|1.8373384|\n",
      "|   7888|Terminator 3: The...|1.7800411|\n",
      "+-------+--------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recommend_for_user(1, 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 188:===========================================>          (80 + 2) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|user_id|   rating|\n",
      "+-------+---------+\n",
      "|   9112|5.0123577|\n",
      "|  41927| 4.878981|\n",
      "|  19187| 4.751763|\n",
      "|  37752|4.6243577|\n",
      "|  32604| 4.590875|\n",
      "|  51908| 4.454318|\n",
      "|  20354|4.4172673|\n",
      "|  39238| 4.409904|\n",
      "|  35474|4.3317375|\n",
      "|   9588|4.2621307|\n",
      "+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recommend_for_item(1, 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
